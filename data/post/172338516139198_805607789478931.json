{"id":"172338516139198_805607789478931","from":{"name":"Mohamed Nady","id":"10154387282503733"},"message":"The Problem: \nA huge SQL Server database for government sector get grows by huge amount of data every 2 months.\nSo they move the completed transactions to backups every 2 months to make the business solution working fine and faster.\nBut when they try to search or making some business upon old transactions they enforced to extract the old transactions from old backups and move it to production DB.\nThe Question: what is the best solution or way to automatically extend the current SQl Server database to multiple database automatically every period time and the custom business solution can deal with them all.","actions":[{"name":"Comment","link":"https://www.facebook.com/172338516139198/posts/805607789478931"},{"name":"Like","link":"https://www.facebook.com/172338516139198/posts/805607789478931"},{"name":"Report Post to Admin","link":"https://www.facebook.com/groups/egyptian.geeks/members/"}],"privacy":{"value":"","description":"","friends":"","allow":"","deny":""},"type":"status","created_time":"2014-08-28T07:13:52+0000","updated_time":"2014-08-28T11:14:21+0000","is_hidden":false,"is_expired":false,"comments":{"data":[{"created_time":"2014-08-28T07:43:36+0000","from":{"name":"Ahmed Shabana Al-Saigh","id":"10154126106536595"},"message":"if you not prefer or not to investment at new HW you can do this enhancement , 1. Aggregate the warehouse data , i.e. use cubes , to make lookup to processed data instead of repeat IO load to it , 2. try to categoraize table that considered to be CDRs and put it at any free schema DB i.e. cassandra or mongo DB , 4. it is very important to make the old lookup query act in its separate space to remove its impact on the production environment","can_remove":false,"like_count":1,"user_likes":false,"id":"805612472811796"},{"created_time":"2014-08-28T07:44:44+0000","from":{"name":"Ahmed Shabana Al-Saigh","id":"10154126106536595"},"message":"if you have problem at working data you can check vertical and horizental partitioning approaches","can_remove":false,"like_count":2,"user_likes":false,"id":"805612632811780"},{"created_time":"2014-08-28T08:28:13+0000","from":{"name":"Mohammad Tayseer","id":"10154965677956341"},"message":"You don't need to put the data back into the production system. Separating the old data from the newer data is called \"Data Warehousing\". Searching & reporting should work only on the data warehouse.\n\nI'm not an expert in data warehousing. You can search more about data warehousing in SQL Server & SSIS.","can_remove":false,"like_count":2,"user_likes":false,"id":"805620139477696"},{"created_time":"2014-08-28T08:38:00+0000","from":{"name":"Mohamed Elsherif","id":"10158483817400314"},"message":"Reading and Writing in Databases are two different optimization problems, and most of the time you can only optimize for one at a time.\nThe most common solution is usually to separate the OLTP database (your typical application database transaction) in a separate DB, and a different Reporting DB that can have a completely different table schema (most common schemas for reporting are Star Schema, and Snow-Flake Schema) and in between you develop regular integration jobs that run regularly usually off working hours time to read all the data that happened since the last time it ran from the OLTP system and transform the data and load it into the reporting DB, this operation is usually referred to as ETL (Extract \\ Transform \\ Load) technologies like SSIS can help you with this.","can_remove":false,"like_count":3,"user_likes":false,"id":"805621696144207"},{"created_time":"2014-08-28T08:41:40+0000","from":{"name":"Mohammed Abdelhay","id":"10154444248108527"},"message":"best solution for That Mohamed Nady is creating multi database files and using part function to split data by date and save each range into its physical database file","can_remove":false,"like_count":1,"message_tags":[{"id":"10154387282503733","length":12,"name":"Mohamed Nady","offset":23,"type":"user"}],"user_likes":false,"id":"805622319477478"},{"created_time":"2014-08-28T08:46:18+0000","from":{"name":"Mohamed Elsherif","id":"10158483817400314"},"message":"One other problem that usually makes reading optimization conflict with writing is that people usually want to search using multiple criteria (especially Dates) so you wish you can put indexes on all columns, but if you are writing in the same db from your live system the more indexes the slower the writes will be, also making long queries on the DB will slow the database, which will slow your live system as well.\nso you get more freedom in optimization by separating the two systems, you can run from completely different hardware, and making indexes on any columns you like and also make any transformation on the data that makes sense for your reporting like denormalizing some data to reduce the unnecessary joins.","can_remove":false,"like_count":2,"user_likes":false,"id":"805623096144067"},{"created_time":"2014-08-28T08:52:54+0000","from":{"name":"Mohamed Alaa El Din","id":"1252478278163674"},"message":"http://www.scalebase.com/resources/database-sharding/\n\nI would recommend using Solr or Elastic Search for Indexing the database automatically and you can ask Solr for whatever you want from the database.","can_remove":false,"like_count":2,"user_likes":false,"id":"805626129477097"},{"created_time":"2014-08-28T11:14:21+0000","from":{"name":"Wael Ali Salim","id":"10158265720905304"},"message":"Well, making business decisions does not necessary require to keep the row transactions data. The system provider can produce a full set of reports showing both service, and system KPIs per day, per service, per department, per city, per governorate, or per whatever can be grouped by. You then can store these reports/summary data for a longer time and drop the historical row data. i.e You keep 2 months of transnational data and 22 months of summary and reporting data. This will reduce the size dramatically and you can even use these summary data to generate a higher level set of reports. You will need to refer to the historical transactions only in case of complains or single specific transaction investigation;","can_remove":false,"like_count":1,"user_likes":false,"id":"805663999473310"}],"paging":{"cursors":{"before":"WTI5dGJXVnVkRjlqZAFhKemIzSTZAPREExTmpFeU5EY3lPREV4TnprMk9qRTBNRGt5TVRFNE1UWT0ZD","after":"WTI5dGJXVnVkRjlqZAFhKemIzSTZAPREExTmpZAek9UazVORGN6TXpFd09qRTBNRGt5TWpRME5qRT0ZD"}}}}