{"id":"172338516139198_1010490118990696","from":{"name":"Mostafa Kasem","id":"1822709031088736"},"message":"I'm getting (Additionally, a 500 Internal Server Error error was encountered while trying to use an ErrorDocument to handle the request) after about 10 minutes from running my php file, The page has a lot of CURL Requests in it, I have all timeouts and max execution time set to 0 :\n\n@ini_set('max_execution_time', '0');\n@ini_set('max_input_time', '0');\n@ini_set('mbstring.func_overload', '0');\n@ini_set('output_handler', '');\n@ini_set('default_socket_timeout','10');\n@ini_set('memory_limit','512M');\n@set_time_limit(0);\n\nAnd still getting the same error.\n\n* This problem only happened when number of CURL Requests raised means it was working till yesterday and I'm pretty sure the code has nothing wrong with it.\n\nany Ideas ?","actions":[{"name":"Comment","link":"https://www.facebook.com/172338516139198/posts/1010490118990696"},{"name":"Like","link":"https://www.facebook.com/172338516139198/posts/1010490118990696"},{"name":"Report Post to Admin","link":"https://www.facebook.com/groups/egyptian.geeks/members/"}],"privacy":{"value":"","description":"","friends":"","allow":"","deny":""},"type":"status","created_time":"2015-09-22T22:22:22+0000","updated_time":"2015-09-23T07:04:30+0000","is_hidden":false,"is_expired":false,"likes":{"data":[{"id":"1264336930310596","name":"Ahmed Ali"}],"paging":{"cursors":{"before":"MTI2NDMzNjkzMDMxMDU5NgZDZD","after":"MTI2NDMzNjkzMDMxMDU5NgZDZD"}}},"comments":{"data":[{"created_time":"2015-09-22T22:26:29+0000","from":{"name":"Morad Edwar","id":"10212482712403270"},"message":"what about running the cURLs in parallel?? search and I think there is a way to do that in PHP.","can_remove":false,"like_count":2,"user_likes":false,"id":"1010490828990625"},{"created_time":"2015-09-22T22:28:14+0000","from":{"name":"Sherif Omar","id":"10158356909500092"},"message":"The timeout can also be at the apache level. Running a PHP script for more than 10 minutes is very poor design. Break it up into tasks and cron it","can_remove":false,"like_count":3,"user_likes":false,"id":"1010491205657254"},{"created_time":"2015-09-22T22:29:20+0000","from":{"name":"Ahmed Al-Amir","id":"10155179919799180"},"message":"Check the logs. Google the error there. Post on StackOverflow.","can_remove":false,"like_count":2,"user_likes":false,"id":"1010491392323902"},{"created_time":"2015-09-22T22:33:09+0000","from":{"name":"Morad Edwar","id":"10212482712403270"},"message":"Also you can split the requests to steps and make ajax request to execute the first step hen the next one in the callback of the ajax request and go on and tell the user about the progress depending on the total requests - the finished .. that's just in case you will use it as a web page not a service file","can_remove":false,"like_count":1,"user_likes":false,"id":"1010492215657153"},{"created_time":"2015-09-23T07:04:30+0000","from":{"name":"Mohamed Hassan Hammam","id":"1935185923377515"},"message":"first , there is fatal error will appear to you && Maximum execution time  will be exceeded >>>\nthe problem in max_execution_time ---> to fetch the data \n -->solution\n ini_set('max_execution_time',240); the time in second \n-----\nif there is  https in the urls , write:\n$opts = array('http'=>array('header' => \"User-Agent:MyAgent/1.0\\r\\n\"));\n$context = stream_context_create($opts);\n$html = file_get_contents(\"https://www.***.com/**\",false,$context);","can_remove":false,"like_count":0,"user_likes":false,"id":"1010628352310206"}],"paging":{"cursors":{"before":"WTI5dGJXVnVkRjlqZAFhKemIzSTZANVEF4TURRNU1EZA3lPRGs1TURZAeU5Ub3hORFF5T1RZAd056a3cZD","after":"WTI5dGJXVnVkRjlqZAFhKemIzSTZANVEF4TURZAeU9ETTFNak14TURJd05qb3hORFF5T1RreE9EY3cZD"}}}}